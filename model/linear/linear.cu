#include <cuda.h>
#include <cuda_runtime.h>
#include <torch/extension.h>
#include <vector>
#include <stdio.h>

// Constants for block dimensions
// Thus, each block stores 64 x 64 x 2 float32 values = 32 KiB of shared memory, which works for A100 GPUs!

// For this project, I am using GPUs from Vast.ai for super cheap :)
#define BLOCK_SIZE 16
#define THREAD_SIZE 4
#define TILE_SIZE 64

#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")

// Forward declaration for our CUDA kernels
__global__ void linear_forward_kernel(
    const float* __restrict__ X,
    const float* __restrict__ weights,
    float* __restrict__ output,
    const int in_features,
    const int out_features);

// Forward pass kernel implementation
__global__ void linear_forward_kernel(
    const float* __restrict__ X,
    const float* __restrict__ weights,
    float* __restrict__ output,
    const int in_features,
    const int out_features) {
    
    // Defining shared memory for the block
    __shared__ float X_shared[TILE_SIZE][TILE_SIZE];
    __shared__ float weights_shared[TILE_SIZE][TILE_SIZE];

    // Calculating the block position in the broader output matrix
    const int block_row = blockIdx.y * TILE_SIZE;
    const int block_col = blockIdx.x * TILE_SIZE;

    // Stores the 2D results generated by our thread in thread-local mem! :)
    float values[THREAD_SIZE][THREAD_SIZE] = {0};

    const int num_tiles = int(in_features / TILE_SIZE);

    #pragma unroll
    for (int tile_index = 0; tile_index < num_tiles; tile_index++) {

        // First, we load in our Shared Memory with strides
        #pragma unroll
        for (int row_offset = 0; row_offset < TILE_SIZE; row_offset += BLOCK_SIZE) {
            
            #pragma unroll
            for (int col_offset = 0; col_offset < TILE_SIZE; col_offset += BLOCK_SIZE) {
                
                // Defining indices for the shared positions and the global positions
                int shared_row = row_offset + threadIdx.y;
                int shared_col = col_offset + threadIdx.x;

                int global_row = block_row + shared_row;
                int global_col = (tile_index * TILE_SIZE) + shared_col;
                
                X_shared[shared_row][shared_col] = X[(global_row * in_features) + global_col];

                // Reusing variables for the weights
                global_row = (tile_index * TILE_SIZE) + shared_row;
                global_col = block_col + shared_col;

                weights_shared[shared_row][shared_col] = weights[(global_row * out_features) + global_col];
            }

            // Ensures all threads in-warp are at the same stage for new data loading
            __syncthreads();
        }

        // Finally, we compute the dot-product for the memory currently loaded in
        #pragma unroll
        for (int k = 0; k < TILE_SIZE; k++){
            
            #pragma unroll
            for (int i = 0; i < THREAD_SIZE; i++) {

                #pragma unroll
                for (int j = 0; j < THREAD_SIZE; j++) {
                    values[i][j] += X_shared[threadIdx.y + i * BLOCK_SIZE][k] * weights_shared[k][threadIdx.x + j * BLOCK_SIZE];
                }
            }
        }
        // Synchronize before loading the next tile
        __syncthreads();
    }

    // Write the final results to the output matrix
    #pragma unroll
    for (int i = 0; i < THREAD_SIZE; i++) {
        int outputRow = block_row + threadIdx.y + i * BLOCK_SIZE;

        #pragma unroll
        for (int j = 0; j < THREAD_SIZE; j++) {
            int outputCol = block_col + threadIdx.x + j * BLOCK_SIZE;
            output[outputRow * out_features + outputCol] = values[i][j];
        }
    }
}


// C++ wrapper for the forward pass

torch::Tensor linear_forward_cuda(
    const torch::Tensor& X,
    const torch::Tensor& weights) {
    
    // Ensure the inputs are on CUDA and are contiguous
    CHECK_CUDA(X);
    CHECK_CUDA(weights);
    CHECK_CONTIGUOUS(X);
    CHECK_CONTIGUOUS(weights);
    
    const int in_features = X.size(0);
    const int out_features = weights.size(1);
    
    // Create an output tensor
    auto output = torch::empty({in_features, out_features}, X.options());
    
    // Define thread block and grid sizes
    dim3 threads(BLOCK_SIZE, BLOCK_SIZE);
    dim3 blocks(out_features / TILE_SIZE, in_features / TILE_SIZE);
    
    // Launch the kernel
    linear_forward_kernel<<<blocks, threads>>>(
        X.data_ptr<float>(),
        weights.data_ptr<float>(),
        output.data_ptr<float>(),
        in_features,
        out_features
    );
    
    // Return the output tensor
    return output;
}
