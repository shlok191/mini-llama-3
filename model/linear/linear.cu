#include <cuda.h>
#include <cuda_runtime.h>
#include <torch/extension.h>
#include <vector>
#include <stdio.h>

// Constants for block dimensions
#define BLOCK_SIZE 16
#define TILE_SIZE 64
#define THREAD_SIZE 4 // Loading in 128 bytes per vectorized loading operation
#define VECTOR_WIDTH 4  

#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")

// Forward declaration for our CUDA kernels
__global__ void linear_forward_kernel(
    const float* __restrict__ X,
    const float* __restrict__ weights,
    float* __restrict__ output,
    const int in_features,
    const int out_features);

// Forward pass kernel implementation
__global__ void linear_forward_kernel(
    const float* __restrict__ X,
    const float* __restrict__ weights,
    float* __restrict__ output,
    const int in_features,
    const int out_features) {
    
    // Defining shared memory for the block
    __shared__ float X_shared[TILE_SIZE][TILE_SIZE];
    __shared__ float weights_shared[TILE_SIZE][TILE_SIZE];

    // Calculating the block position in the broader output matrix
    const int block_row = blockIdx.y * TILE_SIZE;
    const int block_col = blockIdx.x * TILE_SIZE;

    // Storing the 2D results generated by our thread in thread-local memory
    float values[THREAD_SIZE][THREAD_SIZE] = {0};

    const int num_tiles = in_features / TILE_SIZE;

    for (int tile_index = 0; tile_index < num_tiles; tile_index++) {

        // Load data into shared memory
        for (int row_offset = 0; row_offset < TILE_SIZE; row_offset += BLOCK_SIZE) {
            int shared_row = row_offset + threadIdx.y;

            int global_row_X = block_row + shared_row;
            int global_row_W = tile_index * TILE_SIZE + shared_row;

            int shared_col = threadIdx.x * VECTOR_WIDTH;
            int global_col_X = tile_index * TILE_SIZE + shared_col;
            int global_col_W = block_col + shared_col;

            // Reinterpret shared memory pointers as float4*
            float4* X_shared_ptr = reinterpret_cast<float4*>(&X_shared[shared_row][shared_col]);
            float4* weights_shared_ptr = reinterpret_cast<float4*>(&weights_shared[shared_row][shared_col]);

            // Reinterpret global memory pointers as float4*
            const float4* X_global_ptr = reinterpret_cast<const float4*>(&X[global_row_X * in_features + global_col_X]);
            const float4* weights_global_ptr = reinterpret_cast<const float4*>(&weights[global_row_W * out_features + global_col_W]);

            // Load data into shared memory
            *X_shared_ptr = *X_global_ptr;
            *weights_shared_ptr = *weights_global_ptr;
        }

        // Synchronize to ensure all data is loaded
        __syncthreads();

        // Compute the dot-product for the memory currently loaded in
        #pragma unroll
        for (int k = 0; k < TILE_SIZE; k++) {
            
            #pragma unroll
            for (int i = 0; i < THREAD_SIZE; i++) {

                #pragma unroll
                for (int j = 0; j < THREAD_SIZE; j++) {
                    values[i][j] += X_shared[threadIdx.y + i * BLOCK_SIZE][k] * weights_shared[k][threadIdx.x + j * BLOCK_SIZE];
                }
            }
        }

        // Synchronize before loading the next tile
        __syncthreads();
    }

    // Write the final results to the output matrix
    #pragma unroll
    for (int i = 0; i < THREAD_SIZE; i++) {
        
        int outputRow = block_row + threadIdx.y + i * BLOCK_SIZE;
        
        #pragma unroll
        for (int j = 0; j < THREAD_SIZE; j++) {

            int outputCol = block_col + threadIdx.x + j * BLOCK_SIZE;
            output[outputRow * out_features + outputCol] = values[i][j];
        }
    }
}

// C++ wrapper for the forward pass
torch::Tensor linear_forward_cuda(
    const torch::Tensor& X,
    const torch::Tensor& weights) {
    
    // Ensure the inputs are on CUDA and are contiguous
    CHECK_CUDA(X);
    CHECK_CUDA(weights);
    CHECK_CONTIGUOUS(X);
    CHECK_CONTIGUOUS(weights);
    
    const int in_features = X.size(0);
    const int out_features = weights.size(1);
    
    // Create an output tensor
    auto output = torch::empty({in_features, out_features}, X.options());
    
    // Define thread block and grid sizes
    dim3 threads(BLOCK_SIZE, BLOCK_SIZE);
    dim3 blocks(out_features / TILE_SIZE, in_features / TILE_SIZE);
    
    // Launch the kernel
    linear_forward_kernel<<<blocks, threads>>>(
        X.data_ptr<float>(),
        weights.data_ptr<float>(),
        output.data_ptr<float>(),
        in_features,
        out_features
    );
    
    // Return the output tensor
    return output;
}
